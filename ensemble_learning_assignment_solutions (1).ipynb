{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15667bce",
      "metadata": {
        "id": "15667bce"
      },
      "source": [
        "# Ensemble Learning – Assignment Solutions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c9e1e81",
      "metadata": {
        "id": "6c9e1e81"
      },
      "source": [
        "## Theoretical Questions and Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bdef94c",
      "metadata": {
        "id": "6bdef94c"
      },
      "source": [
        "### 1. What is ensemble learning in machine learning?\n",
        "\n",
        "Ensemble learning is a technique where we combine predictions from **multiple base models** (often called base learners) to build a **stronger overall model**. Instead of relying on a single classifier/regressor, we train several models and aggregate their outputs (e.g., by majority vote or averaging).\n",
        "\n",
        "The intuition is that multiple diverse models can **reduce variance and/or bias**, leading to better generalization performance compared to any individual model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb07ff7",
      "metadata": {
        "id": "7bb07ff7"
      },
      "source": [
        "### 2. What are the main types of ensemble techniques?\n",
        "\n",
        "Common types of ensemble techniques are:\n",
        "\n",
        "- **Bagging (Bootstrap Aggregating)**  \n",
        "  Train many models in parallel on different bootstrap samples of the training data and aggregate their predictions (e.g., Random Forest).\n",
        "\n",
        "- **Boosting**  \n",
        "  Train models **sequentially**, where each new model focuses on the errors of the previous ones (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM).\n",
        "\n",
        "- **Stacking (Stacked Generalization)**  \n",
        "  Train different base models and then train a **meta-model** on their predictions.\n",
        "\n",
        "- **Voting / Averaging**  \n",
        "  Combine multiple different models by simple majority voting (classification) or averaging (regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "215f05c0",
      "metadata": {
        "id": "215f05c0"
      },
      "source": [
        "### 3. Explain the key idea behind ensemble techniques.\n",
        "\n",
        "The key idea is: **many weak opinions can make a strong decision**. Each base learner may have limited accuracy or may overfit in different ways, but if they are reasonably accurate and **diverse** (i.e., they make different errors), then combining them can:\n",
        "\n",
        "- Reduce **variance** (bagging),\n",
        "- Reduce **bias** (boosting),\n",
        "- Improve **robustness** and **stability**,\n",
        "- Provide better generalization to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36c82791",
      "metadata": {
        "id": "36c82791"
      },
      "source": [
        "### 4. What is a Random Forest Classifier?\n",
        "\n",
        "A Random Forest Classifier is an ensemble of many **decision trees** trained using **bagging** plus **feature randomness**:\n",
        "\n",
        "- Each tree is trained on a bootstrap sample of the training data.\n",
        "- At each split, the tree considers only a random subset of features.\n",
        "- For classification, predictions from all trees are combined by **majority voting**.\n",
        "\n",
        "This randomness decorrelates the trees and usually gives better performance and robustness than a single decision tree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af366af8",
      "metadata": {
        "id": "af366af8"
      },
      "source": [
        "### 5. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "A single decision tree:\n",
        "- Has **high variance** (it can change a lot with small changes in data).\n",
        "- Tends to **overfit** if not pruned properly.\n",
        "\n",
        "Random Forest:\n",
        "- Averages predictions from many trees, which **reduces variance**.\n",
        "- Uses feature randomness and bootstrap sampling to create **diverse** trees.\n",
        "- Is usually **more accurate, robust, and stable**, and less sensitive to noise/outliers than a single tree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0499c96",
      "metadata": {
        "id": "a0499c96"
      },
      "source": [
        "### 6. What is the main advantage of ensemble techniques?\n",
        "\n",
        "The main advantage is **improved predictive performance**:\n",
        "\n",
        "- Lower generalization error (better accuracy / lower MSE).  \n",
        "- Reduced variance and sometimes reduced bias.  \n",
        "- Improved robustness to noise and outliers.  \n",
        "\n",
        "In practice, ensemble methods often rank among the **top-performing models** in machine learning competitions and real-world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc356a0",
      "metadata": {
        "id": "abc356a0"
      },
      "source": [
        "### 7. What is the main challenge of ensemble methods?\n",
        "\n",
        "Key challenges include:\n",
        "\n",
        "- **Increased computational cost**: Training many models can be slower and more memory intensive.\n",
        "- **Complexity & interpretability**: Ensembles are harder to interpret than a single simple model.\n",
        "- **Tuning**: More hyperparameters (number of estimators, depth, learning rate, etc.) can make tuning more complex.\n",
        "- **Deployment**: Larger model size and latency can be an issue in resource-constrained environments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a15c73bb",
      "metadata": {
        "id": "a15c73bb"
      },
      "source": [
        "### 8. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "Ensemble methods are widely used in:\n",
        "\n",
        "- **Finance**: credit scoring, fraud detection, algorithmic trading.\n",
        "- **Healthcare**: disease prediction, medical image analysis.\n",
        "- **Marketing**: customer churn prediction, recommendation systems.\n",
        "- **Computer vision**: object detection, image classification.\n",
        "- **NLP**: sentiment analysis, spam detection.\n",
        "- **Kaggle/Competition settings**: ensembling is almost standard to boost leaderboard scores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8fd7b16",
      "metadata": {
        "id": "c8fd7b16"
      },
      "source": [
        "### 9. What is the difference between Bagging and Boosting?\n",
        "\n",
        "**Bagging (Bootstrap Aggregating):**\n",
        "- Base learners are trained **in parallel** on different bootstrap samples.  \n",
        "- Aim: mainly **reduce variance**.  \n",
        "- All learners have equal weight in final prediction.  \n",
        "- Example: Random Forest.\n",
        "\n",
        "**Boosting:**\n",
        "- Base learners are trained **sequentially**. Each new learner focuses on **correcting errors** of the previous ones.  \n",
        "- Aim: reduce **bias and variance** by building a strong learner from many weak learners.  \n",
        "- Learners have **different weights** depending on their performance.  \n",
        "- Examples: AdaBoost, Gradient Boosting, XGBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aec3058",
      "metadata": {
        "id": "6aec3058"
      },
      "source": [
        "### 10. How does Bagging help in reducing overfitting?\n",
        "\n",
        "Bagging trains many models on **different bootstrap samples** of the training data and then averages their predictions (or uses majority vote). While an individual high-variance model (like a deep decision tree) may overfit, the **average of many overfitted models is less overfitted** because their errors tend to cancel out.\n",
        "\n",
        "Thus, bagging **reduces variance**, which effectively reduces overfitting and improves generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8059b08",
      "metadata": {
        "id": "a8059b08"
      },
      "source": [
        "### 11. Can we use Bagging for regression problems?\n",
        "\n",
        "Yes. Bagging works for both **classification and regression** problems.\n",
        "\n",
        "- For classification, predictions from base classifiers are combined by **majority voting** or by averaging class probabilities.  \n",
        "- For regression, predictions from base regressors are combined by **averaging** their outputs.\n",
        "\n",
        "In scikit-learn, `BaggingRegressor` is used for regression tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73c2d90",
      "metadata": {
        "id": "b73c2d90"
      },
      "source": [
        "### 12. What is the difference between multiple model training and single model training?\n",
        "\n",
        "- **Single model training**: Train one model on the available data. The performance depends entirely on this model's capacity and inductive bias.\n",
        "\n",
        "- **Multiple model training (ensembles)**: Train **several models** and combine their predictions. This can:\n",
        "  - Reduce variance (bagging),  \n",
        "  - Reduce bias (boosting),  \n",
        "  - Increase robustness and stability,  \n",
        "  - But at the cost of higher computation and lower interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e3fc27b",
      "metadata": {
        "id": "2e3fc27b"
      },
      "source": [
        "### 13. Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "In a Random Forest, at each split of a decision tree, we do **not** consider all features. Instead, we randomly select a **subset of features** and choose the best split only from that subset.\n",
        "\n",
        "Effects:\n",
        "- Increases **diversity** between trees (they see different feature subsets).\n",
        "- Reduces **correlation** between trees, so averaging them is more effective.\n",
        "- Often improves performance and robustness compared to bagging pure decision trees without feature randomness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b58aa9a2",
      "metadata": {
        "id": "b58aa9a2"
      },
      "source": [
        "### 14. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "In bagging-based models like Random Forest:\n",
        "\n",
        "- Each tree is trained on a **bootstrap sample** of the training data.\n",
        "- On average, about **1/3 of the samples are not included** in that bootstrap sample; these are called **Out-of-Bag (OOB) samples**.\n",
        "\n",
        "The **OOB score** is an internal validation score computed by:\n",
        "- For each training sample, aggregating predictions from only the trees that **did not** see that sample during training.  \n",
        "- Comparing these aggregated predictions with true labels.\n",
        "\n",
        "It provides an **unbiased estimate** of the model’s generalization performance without needing a separate validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959775ce",
      "metadata": {
        "id": "959775ce"
      },
      "source": [
        "### 15. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "Common approaches:\n",
        "\n",
        "1. **Impurity-based feature importance** (Gini importance):  \n",
        "   - During training, each split reduces an impurity measure (e.g., Gini or MSE).  \n",
        "   - For each feature, sum the impurity decrease it produces over all trees and normalize.\n",
        "\n",
        "2. **Permutation importance**:  \n",
        "   - After training, randomly shuffle the values of one feature across the dataset.  \n",
        "   - Measure how much model performance drops. Larger drops indicate more important features.\n",
        "\n",
        "Most libraries (like scikit-learn) expose `feature_importances_` for impurity-based importance and implement permutation importance utilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d921b2b4",
      "metadata": {
        "id": "d921b2b4"
      },
      "source": [
        "### 16. Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "Working steps:\n",
        "\n",
        "1. Given a training set, create **B bootstrap samples** (random samples with replacement of the same size as the original dataset).\n",
        "2. Train a **base classifier** (e.g., decision tree, SVM, logistic regression) on each bootstrap sample.\n",
        "3. For a new instance, get predictions from all base classifiers.\n",
        "4. Combine predictions using **majority voting** (for classification) to get the final output.\n",
        "\n",
        "Because each base model sees a different subset of data, their errors are less correlated, and the aggregated prediction is more stable and accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fdb63b5",
      "metadata": {
        "id": "4fdb63b5"
      },
      "source": [
        "### 17. How do you evaluate a Bagging Classifier’s performance?\n",
        "\n",
        "To evaluate a Bagging Classifier, we can use the **same metrics** as for any other classifier:\n",
        "\n",
        "- **Accuracy**  \n",
        "- **Precision, Recall, F1-score**  \n",
        "- **Confusion matrix**  \n",
        "- **ROC-AUC, PR-AUC** (for probabilistic outputs)  \n",
        "\n",
        "We can evaluate performance using:\n",
        "- A **train/test split**\n",
        "- **Cross-validation**\n",
        "- **OOB score** (if available) as an internal estimate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6fde6",
      "metadata": {
        "id": "70e6fde6"
      },
      "source": [
        "### 18. How does a Bagging Regressor work?\n",
        "\n",
        "Bagging Regressor works similarly to Bagging Classifier but for regression:\n",
        "\n",
        "1. Generate multiple bootstrap samples from the training data.\n",
        "2. Train a **base regressor** (e.g., decision tree regressor, KNN regressor) on each sample.\n",
        "3. For a new input, obtain predictions from all regressors.\n",
        "4. Take the **average** of all predictions as the final prediction.\n",
        "\n",
        "Averaging reduces variance and helps the model generalize better."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319c6d91",
      "metadata": {
        "id": "319c6d91"
      },
      "source": [
        "### 19. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "Bootstrap sampling means sampling **with replacement** from the original dataset to create multiple training sets (each of the same size as the original). In Bagging:\n",
        "\n",
        "- Each base model is trained on a **different bootstrap sample**.\n",
        "- This introduces variability and **decorrelates** the models.\n",
        "- Aggregating these diverse models reduces variance and improves generalization.\n",
        "\n",
        "Without bootstrap sampling, many models might become too similar, reducing the benefit of ensembling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c5c4b62",
      "metadata": {
        "id": "8c5c4b62"
      },
      "source": [
        "### 20. When should we avoid using ensemble methods?\n",
        "\n",
        "We might avoid ensembles when:\n",
        "\n",
        "- **Interpretability is crucial**: e.g., regulatory domains where you must explain each decision clearly; a single decision tree or linear model might be preferred.\n",
        "- **Limited computational resources**: ensembles can be slow to train and serve.\n",
        "- **Very small datasets**: a complex ensemble might overfit; a simpler model with good regularization may be better.\n",
        "- **Low-latency requirements**: real-time systems with strict timing constraints may require smaller models.\n",
        "\n",
        "In such cases, a simpler, well-regularized model can be more appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cbde07",
      "metadata": {
        "id": "b6cbde07"
      },
      "source": [
        "## Practical Tasks – Python Implementations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "028e7239",
      "metadata": {
        "id": "028e7239"
      },
      "outputs": [],
      "source": [
        "# Common imports used in most tasks\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, load_iris, fetch_california_housing, make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import (accuracy_score, mean_squared_error, confusion_matrix,\n",
        "                             classification_report, roc_auc_score, roc_curve,\n",
        "                             precision_recall_curve, precision_score, recall_score, f1_score)\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b396c15",
      "metadata": {
        "id": "8b396c15"
      },
      "source": [
        "### 1) Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c912245a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c912245a",
        "outputId": "9002d1d1-5cc4-432f-d0e4-5c9ff4d01b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging DecisionTree accuracy: 0.9333\n"
          ]
        }
      ],
      "source": [
        "# Using the Iris dataset as a sample classification dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "base_clf = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "bagging_clf = BaggingClassifier(estimator=base_clf,\n",
        "                                n_estimators=50,\n",
        "                                random_state=RANDOM_STATE)\n",
        "\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging DecisionTree accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c1716d",
      "metadata": {
        "id": "27c1716d"
      },
      "source": [
        "### 2) Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d8008e2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8008e2e",
        "outputId": "3b09a2d8-eac3-403c-9f18-de451730ba6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging DecisionTree Regressor MSE: 2885.7360\n"
          ]
        }
      ],
      "source": [
        "# Using a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=10.0, random_state=RANDOM_STATE)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
        "\n",
        "base_reg = DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
        "bagging_reg = BaggingRegressor(estimator=base_reg,\n",
        "                               n_estimators=50,\n",
        "                               random_state=RANDOM_STATE)\n",
        "\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging DecisionTree Regressor MSE: {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c301fc",
      "metadata": {
        "id": "e4c301fc"
      },
      "source": [
        "### 3) Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7623c084",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7623c084",
        "outputId": "0e7cc598-e699-48a6-8c3f-a309012fe2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 features by importance:\n",
            "\n",
            "worst concave points: 0.1590\n",
            "worst area: 0.1470\n",
            "worst perimeter: 0.0858\n",
            "worst radius: 0.0790\n",
            "mean radius: 0.0777\n",
            "mean perimeter: 0.0742\n",
            "mean concave points: 0.0659\n",
            "mean concavity: 0.0543\n",
            "mean area: 0.0417\n",
            "worst concavity: 0.0314\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "importances = rf_clf.feature_importances_\n",
        "sorted_idx = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"Top 10 features by importance:\\n\")\n",
        "for idx in sorted_idx[:10]:\n",
        "    print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e19ba26",
      "metadata": {
        "id": "7e19ba26"
      },
      "source": [
        "### 4) Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "471346eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "471346eb",
        "outputId": "bd9c0bbc-cb80-44ad-d08b-cd6df2cd7ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeRegressor MSE: 0.5280\n",
            "RandomForestRegressor MSE: 0.2565\n"
          ]
        }
      ],
      "source": [
        "# Using California housing dataset for regression (or fallback to synthetic)\n",
        "try:\n",
        "    housing = fetch_california_housing()\n",
        "    X, y = housing.data, housing.target\n",
        "except Exception as e:\n",
        "    print(\"Could not fetch California housing, using synthetic data.\")\n",
        "    X, y = make_regression(n_samples=2000, n_features=8, noise=15.0, random_state=RANDOM_STATE)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
        "\n",
        "dt_reg = DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE)\n",
        "\n",
        "dt_reg.fit(X_train, y_train)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "dt_pred = dt_reg.predict(X_test)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(f\"DecisionTreeRegressor MSE: {dt_mse:.4f}\")\n",
        "print(f\"RandomForestRegressor MSE: {rf_mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6549b3a",
      "metadata": {
        "id": "b6549b3a"
      },
      "source": [
        "### 5) Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c7c231b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c7c231b",
        "outputId": "161366d8-bb5d-4f74-fa45-236a7168e7e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9596\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf_clf_oob = RandomForestClassifier(n_estimators=200,\n",
        "                                   oob_score=True,\n",
        "                                   bootstrap=True,\n",
        "                                   random_state=RANDOM_STATE)\n",
        "rf_clf_oob.fit(X, y)\n",
        "\n",
        "print(f\"OOB Score: {rf_clf_oob.oob_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ab5f68",
      "metadata": {
        "id": "75ab5f68"
      },
      "source": [
        "### 6) Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b6bcd57c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6bcd57c",
        "outputId": "7c749f23-694d-4efc-d119-2747eaadb648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging SVM accuracy: 0.9556\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "base_svm = SVC(probability=False, kernel='rbf', gamma='scale', random_state=RANDOM_STATE)\n",
        "bagging_svm = BaggingClassifier(estimator=base_svm,\n",
        "                                n_estimators=20,\n",
        "                                random_state=RANDOM_STATE)\n",
        "\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging SVM accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f895f8b",
      "metadata": {
        "id": "4f895f8b"
      },
      "source": [
        "### 7) Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b50df325",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b50df325",
        "outputId": "1e1eab0a-8929-4491-aa5e-27ca9711f170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_estimators=10: accuracy=0.9298\n",
            "n_estimators=50: accuracy=0.9240\n",
            "n_estimators=100: accuracy=0.9357\n",
            "n_estimators=200: accuracy=0.9415\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "for n in [10, 50, 100, 200]:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=RANDOM_STATE)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"n_estimators={n}: accuracy={acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a8d6f9f",
      "metadata": {
        "id": "8a8d6f9f"
      },
      "source": [
        "### 8) Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f46c287c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f46c287c",
        "outputId": "7aa0d23f-86f2-42ba-ba0f-1f45048dd56c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging LogisticRegression ROC-AUC: 0.9911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "base_log = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "bagging_log = BaggingClassifier(estimator=base_log,\n",
        "                                n_estimators=20,\n",
        "                                random_state=RANDOM_STATE)\n",
        "\n",
        "bagging_log.fit(X_train, y_train)\n",
        "y_proba = bagging_log.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"Bagging LogisticRegression ROC-AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2af5b9a",
      "metadata": {
        "id": "f2af5b9a"
      },
      "source": [
        "### 9) Train a Random Forest Regressor and analyze feature importance scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "58bc0850",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58bc0850",
        "outputId": "2ee67375-b579-4475-d2b0-89a868ae7fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances:\n",
            "\n",
            "MedInc: 0.5260\n",
            "AveOccup: 0.1382\n",
            "Longitude: 0.0861\n",
            "Latitude: 0.0861\n",
            "HouseAge: 0.0547\n",
            "AveRooms: 0.0472\n",
            "Population: 0.0317\n",
            "AveBedrms: 0.0300\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    housing = fetch_california_housing()\n",
        "    X, y = housing.data, housing.target\n",
        "    feature_names = housing.feature_names\n",
        "except Exception as e:\n",
        "    print(\"Could not fetch California housing, using synthetic data.\")\n",
        "    X, y = make_regression(n_samples=2000, n_features=8, noise=15.0, random_state=RANDOM_STATE)\n",
        "    feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "importances = rf_reg.feature_importances_\n",
        "sorted_idx = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"Feature importances:\\n\")\n",
        "for idx in sorted_idx:\n",
        "    print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cecb634",
      "metadata": {
        "id": "6cecb634"
      },
      "source": [
        "### 10) Train an ensemble model using both Bagging and Random Forest and compare accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ef08d3a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef08d3a7",
        "outputId": "b878deb5-5e49-4e95-85f3-f51eea8743ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaggingClassifier accuracy: 0.9415\n",
            "RandomForestClassifier accuracy: 0.9357\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "# Bagging with decision trees\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "                                n_estimators=50,\n",
        "                                random_state=RANDOM_STATE)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_acc = accuracy_score(y_test, bagging_clf.predict(X_test))\n",
        "\n",
        "# Random Forest\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_acc = accuracy_score(y_test, rf_clf.predict(X_test))\n",
        "\n",
        "print(f\"BaggingClassifier accuracy: {bagging_acc:.4f}\")\n",
        "print(f\"RandomForestClassifier accuracy: {rf_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f7e468",
      "metadata": {
        "id": "d9f7e468"
      },
      "source": [
        "## Additional Practical Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa88ef74",
      "metadata": {
        "id": "fa88ef74"
      },
      "source": [
        "### 11) Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b4ea647f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ea647f",
        "outputId": "434ed0bf-1622-417d-82ed-edd9ecefe3ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'max_depth': 5, 'max_features': 'log2', 'n_estimators': 50}\n",
            "Best CV accuracy: 0.9673995595048227\n",
            "Test accuracy: 0.9473684210526315\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV accuracy:\", grid.best_score_)\n",
        "\n",
        "best_rf = grid.best_estimator_\n",
        "test_acc = accuracy_score(y_test, best_rf.predict(X_test))\n",
        "print(\"Test accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b5ea658",
      "metadata": {
        "id": "3b5ea658"
      },
      "source": [
        "### 12) Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "62f8ba25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62f8ba25",
        "outputId": "44d19262-58bf-4314-8b57-d6664bdde282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_estimators=10: MSE=3370.7124\n",
            "n_estimators=30: MSE=3053.2795\n",
            "n_estimators=50: MSE=2885.7360\n",
            "n_estimators=100: MSE=2865.7296\n"
          ]
        }
      ],
      "source": [
        "X, y = make_regression(n_samples=1000, n_features=10, noise=10.0, random_state=RANDOM_STATE)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
        "\n",
        "for n in [10, 30, 50, 100]:\n",
        "    bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
        "                               n_estimators=n,\n",
        "                               random_state=RANDOM_STATE)\n",
        "    bag_reg.fit(X_train, y_train)\n",
        "    y_pred = bag_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"n_estimators={n}: MSE={mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0524b24f",
      "metadata": {
        "id": "0524b24f"
      },
      "source": [
        "### 13) Train a Random Forest Classifier and analyze misclassified samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "10c01a67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10c01a67",
        "outputId": "84ea3092-814a-4f03-be0f-1ed478c277df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of misclassified samples: 5\n",
            "\n",
            "Indices and (true, pred) labels:\n",
            "Index 2: true=2, pred=1\n",
            "Index 24: true=2, pred=1\n",
            "Index 30: true=2, pred=1\n",
            "Index 39: true=1, pred=2\n",
            "Index 42: true=2, pred=1\n"
          ]
        }
      ],
      "source": [
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "mis_idx = np.where(y_pred != y_test)[0]\n",
        "print(f\"Number of misclassified samples: {len(mis_idx)}\\n\")\n",
        "print(\"Indices and (true, pred) labels:\")\n",
        "for i in mis_idx:\n",
        "    print(f\"Index {i}: true={y_test[i]}, pred={y_pred[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "becc2fa9",
      "metadata": {
        "id": "becc2fa9"
      },
      "source": [
        "### 14) Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "81a6e992",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81a6e992",
        "outputId": "ea1d92f6-4856-4282-f3dd-f9d583550e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree accuracy: 0.9181\n",
            "Bagging DecisionTree accuracy: 0.9415\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "bagging_dt = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "                               n_estimators=50,\n",
        "                               random_state=RANDOM_STATE)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "bag_acc = accuracy_score(y_test, bagging_dt.predict(X_test))\n",
        "\n",
        "print(f\"DecisionTree accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging DecisionTree accuracy: {bag_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ddd6fbf",
      "metadata": {
        "id": "2ddd6fbf"
      },
      "source": [
        "### 15) Train a Random Forest Classifier and visualize the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "353b5f1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "353b5f1a",
        "outputId": "e92137cb-cfdc-4b75-cdd0-c07141d1255e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            " [[ 58   6]\n",
            " [  5 102]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(np.unique(y)))\n",
        "plt.xticks(tick_marks, tick_marks)\n",
        "plt.yticks(tick_marks, tick_marks)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3838446c",
      "metadata": {
        "id": "3838446c"
      },
      "source": [
        "### 16) Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d44d975b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d44d975b",
        "outputId": "f344dcb3-3d5a-4135-fc88-287acb14b3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking classifier accuracy: 0.9298\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE)),\n",
        "    ('svm', SVC(probability=True, kernel='rbf', gamma='scale', random_state=RANDOM_STATE))\n",
        "]\n",
        "\n",
        "final_estimator = LogisticRegression(max_iter=500)\n",
        "\n",
        "stack_clf = StackingClassifier(estimators=estimators,\n",
        "                               final_estimator=final_estimator,\n",
        "                               passthrough=False)\n",
        "\n",
        "stack_clf.fit(X_train, y_train)\n",
        "stack_acc = accuracy_score(y_test, stack_clf.predict(X_test))\n",
        "print(f\"Stacking classifier accuracy: {stack_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff96948",
      "metadata": {
        "id": "3ff96948"
      },
      "source": [
        "### 17) Train a Random Forest Classifier and print the top 5 most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "50cc527d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50cc527d",
        "outputId": "3b01c197-7e9c-48ce-f59b-915d2ad144e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 features:\n",
            "\n",
            "worst concave points: 0.1590\n",
            "worst area: 0.1470\n",
            "worst perimeter: 0.0858\n",
            "worst radius: 0.0790\n",
            "mean radius: 0.0777\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "sorted_idx = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"Top 5 features:\\n\")\n",
        "for idx in sorted_idx[:5]:\n",
        "    print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f61d3021",
      "metadata": {
        "id": "f61d3021"
      },
      "source": [
        "### 18) Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9b827f81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b827f81",
        "outputId": "182262c9-7785-4094-a12e-2b890aa44ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9450\n",
            "Recall: 0.9626\n",
            "F1-score: 0.9537\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "                            n_estimators=50,\n",
        "                            random_state=RANDOM_STATE)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de82a131",
      "metadata": {
        "id": "de82a131"
      },
      "source": [
        "### 19) Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1c45c9e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c45c9e1",
        "outputId": "92ec771c-134d-4bc5-e824-2d328a496d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth=None: accuracy=0.9357\n",
            "max_depth=3: accuracy=0.9357\n",
            "max_depth=5: accuracy=0.9415\n",
            "max_depth=8: accuracy=0.9357\n",
            "max_depth=12: accuracy=0.9357\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "depths = [None, 3, 5, 8, 12]\n",
        "accuracies = []\n",
        "\n",
        "for d in depths:\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=d, random_state=RANDOM_STATE)\n",
        "    rf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "    accuracies.append(acc)\n",
        "    print(f\"max_depth={d}: accuracy={acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33afb469",
      "metadata": {
        "id": "33afb469"
      },
      "source": [
        "### 20) Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "261531f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "261531f6",
        "outputId": "aa311354-ca04-4628-ab58-536deb33005f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging DecisionTreeRegressor MSE: 2885.7360\n",
            "Bagging KNeighborsRegressor MSE: 4147.9917\n"
          ]
        }
      ],
      "source": [
        "X, y = make_regression(n_samples=1000, n_features=10, noise=10.0, random_state=RANDOM_STATE)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
        "\n",
        "dt_bag = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
        "                          n_estimators=50,\n",
        "                          random_state=RANDOM_STATE)\n",
        "knn_bag = BaggingRegressor(estimator=KNeighborsRegressor(),\n",
        "                           n_estimators=50,\n",
        "                           random_state=RANDOM_STATE)\n",
        "\n",
        "dt_bag.fit(X_train, y_train)\n",
        "knn_bag.fit(X_train, y_train)\n",
        "\n",
        "dt_mse = mean_squared_error(y_test, dt_bag.predict(X_test))\n",
        "knn_mse = mean_squared_error(y_test, knn_bag.predict(X_test))\n",
        "\n",
        "print(f\"Bagging DecisionTreeRegressor MSE: {dt_mse:.4f}\")\n",
        "print(f\"Bagging KNeighborsRegressor MSE: {knn_mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21540f35",
      "metadata": {
        "id": "21540f35"
      },
      "source": [
        "### 21) Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "dd6a11aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6a11aa",
        "outputId": "114428b5-f892-44d3-86d3-1241957dfea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.9913\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b5e613",
      "metadata": {
        "id": "14b5e613"
      },
      "source": [
        "### 22) Train a Bagging Classifier and evaluate its performance using cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "fc666e03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc666e03",
        "outputId": "d9be1792-f6d6-4c2e-b6e3-a59a86fd4c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation accuracy scores: [0.9122807  0.92105263 0.98245614 0.95614035 1.        ]\n",
            "Mean accuracy: 0.9543859649122808\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "                            n_estimators=50,\n",
        "                            random_state=RANDOM_STATE)\n",
        "\n",
        "scores = cross_val_score(bag_clf, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-validation accuracy scores:\", scores)\n",
        "print(\"Mean accuracy:\", scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef01e8c",
      "metadata": {
        "id": "7ef01e8c"
      },
      "source": [
        "### 23) Train a Random Forest Classifier and plot the Precision-Recall curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f2104af7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "f2104af7",
        "outputId": "047eb0a4-29b8-40a4-e984-9def4edc4eaa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve (Random Forest)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c62665",
      "metadata": {
        "id": "90c62665"
      },
      "source": [
        "### 24) Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7666a62d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7666a62d",
        "outputId": "556b021e-b80e-414d-913b-59872ca9b090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking (RF + LR) accuracy: 0.9357\n"
          ]
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
        "]\n",
        "\n",
        "final_estimator = LogisticRegression(max_iter=500)\n",
        "\n",
        "stack_clf = StackingClassifier(estimators=estimators,\n",
        "                               final_estimator=final_estimator,\n",
        "                               passthrough=False)\n",
        "\n",
        "stack_clf.fit(X_train, y_train)\n",
        "stack_acc = accuracy_score(y_test, stack_clf.predict(X_test))\n",
        "print(f\"Stacking (RF + LR) accuracy: {stack_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb7c189b",
      "metadata": {
        "id": "eb7c189b"
      },
      "source": [
        "### 25) Train a Bagging Regressor with different levels of bootstrap samples and compare performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "251db48b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "251db48b",
        "outputId": "615c5152-63c4-44f6-c19c-d3c52f2c0480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_samples=0.5: MSE=3203.4483\n",
            "max_samples=0.7: MSE=3163.6627\n",
            "max_samples=1.0: MSE=2885.7360\n"
          ]
        }
      ],
      "source": [
        "# bootstrap=True always samples the same size as original dataset, but max_samples can change effective sample size\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=10.0, random_state=RANDOM_STATE)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
        "\n",
        "for frac in [0.5, 0.7, 1.0]:\n",
        "    bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
        "                               n_estimators=50,\n",
        "                               max_samples=frac,\n",
        "                               bootstrap=True,\n",
        "                               random_state=RANDOM_STATE)\n",
        "    bag_reg.fit(X_train, y_train)\n",
        "    y_pred = bag_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"max_samples={frac}: MSE={mse:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}